{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c026c6",
   "metadata": {},
   "source": [
    "# SmartDynamic: Demand Forecasting Models\n",
    "\n",
    "This notebook implements different demand forecasting models for the SmartDynamic pricing system. We'll develop and compare multiple approaches including statistical models, machine learning models, and deep learning techniques to predict product demand.\n",
    "\n",
    "## Objectives\n",
    "- Develop time series forecasting models using Prophet\n",
    "- Implement LSTM-based deep learning models\n",
    "- Create XGBoost regression models\n",
    "- Compare model performance\n",
    "- Evaluate models on different product categories\n",
    "- Integrate external factors like weather and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab6953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\PROJECTS\\Data Science\\Smart Dynamic Pricing System\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Machine learning and time series libraries\n",
    "from prophet import Prophet\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Add parent directory to path to enable imports from src\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2883f",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n",
    "\n",
    "First, we'll load the product data and prepare it for forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product data\n",
    "product_data_path = '../data/sample_product_data.csv'\n",
    "products_df = pd.read_csv(product_data_path)\n",
    "\n",
    "# Convert date to datetime\n",
    "products_df['date'] = pd.to_datetime(products_df['date'])\n",
    "products_df = products_df.sort_values('date')\n",
    "\n",
    "# Load weather data for context features\n",
    "weather_data_path = '../data/weather_events_data.csv'\n",
    "weather_df = pd.read_csv(weather_data_path)\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "\n",
    "# Merge product data with New York weather data\n",
    "ny_weather = weather_df[weather_df['location'] == 'New York']\n",
    "merged_df = pd.merge(\n",
    "    products_df,\n",
    "    ny_weather[['date', 'temperature', 'precipitation', 'weather_condition', 'event']],\n",
    "    on='date',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {merged_df.shape}\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea885f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for a specific product\n",
    "def prepare_product_data(df, product_id):\n",
    "    \"\"\"\n",
    "    Prepare data for a specific product.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing all products\n",
    "        product_id: ID of the product to prepare data for\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with relevant features for the specified product\n",
    "    \"\"\"\n",
    "    # Filter data for the specific product\n",
    "    product_df = df[df['product_id'] == product_id].copy()\n",
    "    \n",
    "    # Create day of week feature (0 = Monday, 6 = Sunday)\n",
    "    product_df['day_of_week'] = product_df['date'].dt.dayofweek\n",
    "    \n",
    "    # Create week of year feature\n",
    "    product_df['week_of_year'] = product_df['date'].dt.isocalendar().week\n",
    "    \n",
    "    # Create month feature\n",
    "    product_df['month'] = product_df['date'].dt.month\n",
    "    \n",
    "    # Create weekend flag\n",
    "    product_df['is_weekend'] = product_df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Create holiday/event flag\n",
    "    product_df['is_holiday_event'] = (~product_df['event'].isin(['None', 'Weekend'])).astype(int)\n",
    "    \n",
    "    # Create one-hot encoding for weather condition\n",
    "    weather_dummies = pd.get_dummies(product_df['weather_condition'], prefix='weather')\n",
    "    product_df = pd.concat([product_df, weather_dummies], axis=1)\n",
    "    \n",
    "    # Drop non-numeric columns that won't be used in the model\n",
    "    product_df = product_df.drop(['weather_condition', 'event', 'location'], axis=1)\n",
    "    \n",
    "    return product_df\n",
    "\n",
    "# Prepare data for each product\n",
    "product_ids = merged_df['product_id'].unique()\n",
    "product_dataframes = {product_id: prepare_product_data(merged_df, product_id) for product_id in product_ids}\n",
    "\n",
    "# Show prepared data for the first product\n",
    "product_dataframes[product_ids[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ecee2",
   "metadata": {},
   "source": [
    "## 2. Prophet Forecasting Model\n",
    "\n",
    "Facebook Prophet is a robust time series forecasting model that works well with seasonal data and can handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ce15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Prophet model for a product\n",
    "def build_prophet_model(product_df, product_id, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Build and evaluate a Prophet model for a specific product.\n",
    "    \n",
    "    Args:\n",
    "        product_df: DataFrame for the specific product\n",
    "        product_id: Product ID\n",
    "        test_size: Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing model, metrics, and forecasts\n",
    "    \"\"\"\n",
    "    # Prepare data for Prophet (requires 'ds' for dates and 'y' for target variable)\n",
    "    prophet_df = product_df[['date', 'sales_velocity']].rename(\n",
    "        columns={'date': 'ds', 'sales_velocity': 'y'}\n",
    "    )\n",
    "    \n",
    "    # Add price as a regressor\n",
    "    prophet_df['price'] = product_df['current_price']\n",
    "    \n",
    "    # Add temperature as a regressor\n",
    "    prophet_df['temperature'] = product_df['temperature']\n",
    "    \n",
    "    # Add precipitation as a regressor\n",
    "    prophet_df['precipitation'] = product_df['precipitation']\n",
    "    \n",
    "    # Add promotion flag as a regressor\n",
    "    prophet_df['promotion'] = product_df['promotion_flag']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_size = int(len(prophet_df) * (1 - test_size))\n",
    "    train_df = prophet_df.iloc[:train_size]\n",
    "    test_df = prophet_df.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Training Prophet model for Product {product_id}\")\n",
    "    print(f\"Training data: {len(train_df)} records\")\n",
    "    print(f\"Testing data: {len(test_df)} records\")\n",
    "    \n",
    "    # Create and train Prophet model\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=False,  # Not enough data for yearly seasonality\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative'  # Multiplicative seasonality often works better for retail\n",
    "    )\n",
    "    \n",
    "    # Add regressors\n",
    "    model.add_regressor('price')\n",
    "    model.add_regressor('temperature')\n",
    "    model.add_regressor('precipitation')\n",
    "    model.add_regressor('promotion')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(train_df)\n",
    "    \n",
    "    # Make future dataframe for predictions\n",
    "    future = model.make_future_dataframe(periods=len(test_df))\n",
    "    future['price'] = list(prophet_df['price']) + list(test_df['price'])\n",
    "    future['temperature'] = list(prophet_df['temperature']) + list(test_df['temperature'])\n",
    "    future['precipitation'] = list(prophet_df['precipitation']) + list(test_df['precipitation'])\n",
    "    future['promotion'] = list(prophet_df['promotion']) + list(test_df['promotion'])\n",
    "    \n",
    "    # Make predictions\n",
    "    forecast = model.predict(future)\n",
    "    \n",
    "    # Evaluate model on test set\n",
    "    predictions = forecast.iloc[-len(test_df):]['yhat'].values\n",
    "    actuals = test_df['y'].values\n",
    "    \n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    print(f\"Model Evaluation Metrics:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R²: {r2:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'forecast': forecast,\n",
    "        'metrics': {\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        },\n",
    "        'train': train_df,\n",
    "        'test': test_df,\n",
    "        'predictions': predictions,\n",
    "        'actuals': actuals\n",
    "    }\n",
    "\n",
    "# Build Prophet models for each product\n",
    "prophet_results = {}\n",
    "for product_id, product_df in product_dataframes.items():\n",
    "    prophet_results[product_id] = build_prophet_model(product_df, product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e347c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forecasts\n",
    "for product_id, result in prophet_results.items():\n",
    "    model = result['model']\n",
    "    forecast = result['forecast']\n",
    "    train_df = result['train']\n",
    "    test_df = result['test']\n",
    "    \n",
    "    # Plot forecast\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.scatter(train_df['ds'], train_df['y'], color='blue', label='Training Data', alpha=0.6)\n",
    "    \n",
    "    # Plot test data\n",
    "    plt.scatter(test_df['ds'], test_df['y'], color='green', label='Test Data', alpha=0.6)\n",
    "    \n",
    "    # Plot forecast\n",
    "    plt.plot(forecast['ds'], forecast['yhat'], color='red', label='Forecast')\n",
    "    plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], \n",
    "                     color='red', alpha=0.1, label='Forecast Interval')\n",
    "    \n",
    "    plt.title(f'Prophet Forecast for Product {product_id}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales Velocity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot components\n",
    "    fig = model.plot_components(forecast)\n",
    "    plt.suptitle(f'Forecast Components for Product {product_id}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot actual vs predicted for test set\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(test_df['ds'], test_df['y'], color='blue', label='Actual', alpha=0.6)\n",
    "    plt.scatter(test_df['ds'], result['predictions'], color='red', label='Predicted', alpha=0.6)\n",
    "    plt.title(f'Actual vs Predicted Sales Velocity for Product {product_id} (Test Set)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales Velocity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b277f8",
   "metadata": {},
   "source": [
    "## 3. LSTM Deep Learning Model\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b20108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for LSTM model\n",
    "def prepare_lstm_data(df, target_col='sales_velocity', sequence_length=3, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model by creating sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing time series data\n",
    "        target_col: Column name of the target variable\n",
    "        sequence_length: Number of previous time steps to use as input features\n",
    "        test_size: Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing prepared data and scalers\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Select relevant numerical columns (exclude date and categorical columns)\n",
    "    numerical_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    # Scale features\n",
    "    data_scaled = data[numerical_cols].copy()\n",
    "    feature_cols = [col for col in numerical_cols if col != target_col]\n",
    "    \n",
    "    data_scaled[feature_cols] = scaler_X.fit_transform(data[feature_cols])\n",
    "    data_scaled[target_col] = scaler_y.fit_transform(data[[target_col]])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_scaled) - sequence_length):\n",
    "        X.append(data_scaled[feature_cols].values[i:(i + sequence_length)])\n",
    "        y.append(data_scaled[target_col].values[i + sequence_length])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_size = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'feature_cols': feature_cols,\n",
    "        'sequence_length': sequence_length\n",
    "    }\n",
    "\n",
    "# Prepare LSTM data for each product\n",
    "lstm_data = {}\n",
    "for product_id, product_df in product_dataframes.items():\n",
    "    print(f\"\\nPreparing LSTM data for Product {product_id}\")\n",
    "    lstm_data[product_id] = prepare_lstm_data(product_df, sequence_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c236a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and train LSTM model\n",
    "def build_lstm_model(data, product_id):\n",
    "    \"\"\"\n",
    "    Build and train LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing prepared data and scalers\n",
    "        product_id: Product ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing model, history, and metrics\n",
    "    \"\"\"\n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    # Get input shape\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    print(f\"\\nBuilding LSTM model for Product {product_id}\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Testing Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions\n",
    "    y_train_pred_inv = data['scaler_y'].inverse_transform(y_train_pred)\n",
    "    y_test_pred_inv = data['scaler_y'].inverse_transform(y_test_pred)\n",
    "    y_train_inv = data['scaler_y'].inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_test_inv = data['scaler_y'].inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train_inv, y_train_pred_inv)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train_inv, y_train_pred_inv))\n",
    "    test_mae = mean_absolute_error(y_test_inv, y_test_pred_inv)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_inv, y_test_pred_inv))\n",
    "    \n",
    "    print(f\"Train MAE: {train_mae:.2f}\")\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_pred': y_train_pred_inv,\n",
    "        'test_pred': y_test_pred_inv,\n",
    "        'train_actual': y_train_inv,\n",
    "        'test_actual': y_test_inv,\n",
    "        'metrics': {\n",
    "            'train_mae': train_mae,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Build LSTM models for each product\n",
    "lstm_results = {}\n",
    "for product_id, data in lstm_data.items():\n",
    "    lstm_results[product_id] = build_lstm_model(data, product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd02055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM results\n",
    "for product_id, result in lstm_results.items():\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(result['history'].history['loss'], label='Training Loss')\n",
    "    plt.plot(result['history'].history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'LSTM Loss History for Product {product_id}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predictions vs actual for test set\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Get corresponding dates for test set\n",
    "    product_df = product_dataframes[product_id]\n",
    "    sequence_length = lstm_data[product_id]['sequence_length']\n",
    "    test_size = len(lstm_data[product_id]['y_test'])\n",
    "    test_dates = product_df['date'].iloc[-(test_size):].reset_index(drop=True)\n",
    "    \n",
    "    plt.plot(test_dates, result['test_actual'], 'b-', label='Actual', linewidth=2)\n",
    "    plt.plot(test_dates, result['test_pred'], 'r--', label='Predicted', linewidth=2)\n",
    "    \n",
    "    plt.title(f'LSTM Predictions vs Actual for Product {product_id} (Test Set)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales Velocity')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136db822",
   "metadata": {},
   "source": [
    "## 4. XGBoost Time Series Forecasting\n",
    "\n",
    "XGBoost is a powerful gradient boosting algorithm that can be adapted for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b34b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for XGBoost model\n",
    "def prepare_xgboost_data(df, target_col='sales_velocity', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for XGBoost model.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing time series data\n",
    "        target_col: Column name of the target variable\n",
    "        test_size: Proportion of data to use for testing\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing prepared data\n",
    "    \"\"\"\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in range(1, 4):  # Create 3 lag features\n",
    "        data[f'sales_lag_{lag}'] = data['sales_velocity'].shift(lag)\n",
    "    \n",
    "    # Create rolling statistics\n",
    "    data['rolling_mean_3'] = data['sales_velocity'].rolling(window=3).mean()\n",
    "    data['rolling_std_3'] = data['sales_velocity'].rolling(window=3).std()\n",
    "    \n",
    "    # Drop rows with NaN values (due to lag/rolling features)\n",
    "    data = data.dropna()\n",
    "    \n",
    "    # Select features and target\n",
    "    feature_cols = [col for col in data.columns if col != target_col and col != 'date']\n",
    "    X = data[feature_cols]\n",
    "    y = data[target_col]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_size = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "    \n",
    "    # Get corresponding dates\n",
    "    train_dates = data['date'].iloc[:train_size].reset_index(drop=True)\n",
    "    test_dates = data['date'].iloc[train_size:].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_dates': train_dates,\n",
    "        'test_dates': test_dates,\n",
    "        'feature_cols': feature_cols\n",
    "    }\n",
    "\n",
    "# Prepare XGBoost data for each product\n",
    "xgb_data = {}\n",
    "for product_id, product_df in product_dataframes.items():\n",
    "    print(f\"\\nPreparing XGBoost data for Product {product_id}\")\n",
    "    xgb_data[product_id] = prepare_xgboost_data(product_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be226e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build and train XGBoost model\n",
    "def build_xgboost_model(data, product_id):\n",
    "    \"\"\"\n",
    "    Build and train XGBoost model.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary containing prepared data\n",
    "        product_id: Product ID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing model, predictions, and metrics\n",
    "    \"\"\"\n",
    "    X_train = data['X_train']\n",
    "    X_test = data['X_test']\n",
    "    y_train = data['y_train']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "    print(f\"\\nTraining XGBoost model for Product {product_id}\")\n",
    "    \n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': 100\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Train MAE: {train_mae:.2f}\")\n",
    "    print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Test R²: {test_r2:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_pred': y_train_pred,\n",
    "        'test_pred': y_test_pred,\n",
    "        'metrics': {\n",
    "            'train_mae': train_mae,\n",
    "            'train_rmse': train_rmse,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2\n",
    "        },\n",
    "        'feature_importance': model.feature_importances_,\n",
    "        'feature_names': data['feature_cols']\n",
    "    }\n",
    "\n",
    "# Build XGBoost models for each product\n",
    "xgb_results = {}\n",
    "for product_id, data in xgb_data.items():\n",
    "    xgb_results[product_id] = build_xgboost_model(data, product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97048db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize XGBoost results\n",
    "for product_id, result in xgb_results.items():\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sorted_idx = np.argsort(result['feature_importance'])\n",
    "    plt.barh(range(len(sorted_idx)), result['feature_importance'][sorted_idx])\n",
    "    plt.yticks(range(len(sorted_idx)), [result['feature_names'][i] for i in sorted_idx])\n",
    "    plt.title(f'Feature Importance for Product {product_id}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predictions vs actual for test set\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    test_dates = xgb_data[product_id]['test_dates']\n",
    "    \n",
    "    plt.plot(test_dates, xgb_data[product_id]['y_test'], 'b-', label='Actual', linewidth=2)\n",
    "    plt.plot(test_dates, result['test_pred'], 'r--', label='Predicted', linewidth=2)\n",
    "    \n",
    "    plt.title(f'XGBoost Predictions vs Actual for Product {product_id} (Test Set)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales Velocity')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
